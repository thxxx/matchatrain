{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f4bc0-c28e-4670-9a5e-4c7928ab8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5876c0-b47e-4c80-9e9c-62550f81b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hifigan imports\n",
    "from matcha.hifigan.config import v1\n",
    "from matcha.hifigan.denoiser import Denoiser\n",
    "from matcha.hifigan.env import AttrDict\n",
    "from matcha.hifigan.models import Generator as HiFiGAN\n",
    "# Matcha imports\n",
    "from matcha.models.matcha_tts import MatchaTTS\n",
    "from matcha.text import sequence_to_text, text_to_sequence\n",
    "from matcha.utils.model import denormalize\n",
    "from matcha.utils.utils import get_user_data_dir, intersperse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a30306-588c-4f22-8d9b-e2676880b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# This allows for real time code changes being reflected in the notebook, no need to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312856b-01a9-4d75-a4c8-4666dffa0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3b3c3-d014-443b-84eb-e143cdec3e21",
   "metadata": {},
   "source": [
    "## Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640a4c1-44ce-447c-a8ff-45012fb7bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCHA_CHECKPOINT = \"/workspace/matcha_ljspeech.ckpt\"\n",
    "OUTPUT_FOLDER = \"synth_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c8648-b072-4714-b56f-61f4e97bdd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "\n",
    "def load_hparams_from_ckpt(ckpt_path: str):\n",
    "    # 1) 안전 목록에 OmegaConf 타입 허용\n",
    "    torch.serialization.add_safe_globals([DictConfig, ListConfig])\n",
    "    try:\n",
    "        # 2) 우선 안전하게 weights_only=True로 시도\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "    except Exception as e1:\n",
    "        # 3) 신뢰 가능한 파일일 때만 전체 언피클 허용\n",
    "        #    (임의 코드 실행 위험 있으니 출처가 본인/신뢰 가능일 때만!)\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "    # 4) 다양한 키 호환\n",
    "    hparams = (\n",
    "        ckpt.get(\"hyper_parameters\")\n",
    "        or ckpt.get(\"hparams\")\n",
    "        or ckpt.get(\"hparams_initial\")\n",
    "        or {}\n",
    "    )\n",
    "\n",
    "    # 5) DictConfig -> dict\n",
    "    if isinstance(hparams, (DictConfig, ListConfig)):\n",
    "        hparams = OmegaConf.to_container(hparams, resolve=True)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "# === 사용 예시 ===\n",
    "hparams = load_hparams_from_ckpt(MATCHA_CHECKPOINT)\n",
    "\n",
    "# hparams 내부를 한 번 확인(필요하면)\n",
    "# print(hparams)\n",
    "\n",
    "# 보통 이렇게 바로 인자로 들어갑니다.\n",
    "model_empty = MatchaTTS(**hparams)   # 가중치 로드 안 함\n",
    "model_empty.eval()\n",
    "print(f\"Empty model! Parameter count: {count_params(model_empty)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e374bfd2-6c94-48e0-ba44-7875c99a034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    model = MatchaTTS.load_from_checkpoint(checkpoint_path, map_location=device)\n",
    "    model.eval()\n",
    "    return model\n",
    "count_params = lambda x: f\"{sum(p.numel() for p in x.parameters()):,}\"\n",
    "\n",
    "model = load_model(MATCHA_CHECKPOINT)\n",
    "print(f\"Model loaded! Parameter count: {count_params(model)}\") # 18.2M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077b84b-e3b6-42e1-a84b-2f7084b13f92",
   "metadata": {},
   "source": [
    "## Load HiFi-GAN (Vocoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b68184-968d-4868-9029-f0c40e9e68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocos import Vocos\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "import torch\n",
    "\n",
    "# load vocoder\n",
    "def load_vocoder(is_local=False, local_path=\"\", device=\"cuda\", hf_cache_dir=None):\n",
    "    # vocoder = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(device)\n",
    "    if is_local:\n",
    "        print(f\"Load vocos from local path {local_path}\")\n",
    "        config_path = f\"{local_path}/config.yaml\"\n",
    "        model_path = f\"{local_path}/pytorch_model.bin\"\n",
    "    else:\n",
    "        print(\"Download Vocos from huggingface charactr/vocos-mel-24khz\")\n",
    "        repo_id = \"charactr/vocos-mel-24khz\"\n",
    "        config_path = hf_hub_download(repo_id=repo_id, cache_dir=hf_cache_dir, filename=\"config.yaml\")\n",
    "        model_path = hf_hub_download(repo_id=repo_id, cache_dir=hf_cache_dir, filename=\"pytorch_model.bin\")\n",
    "    vocoder = Vocos.from_hparams(config_path)\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
    "    from vocos.feature_extractors import EncodecFeatures\n",
    "\n",
    "    if isinstance(vocoder.feature_extractor, EncodecFeatures):\n",
    "        encodec_parameters = {\n",
    "            \"feature_extractor.encodec.\" + key: value\n",
    "            for key, value in vocoder.feature_extractor.encodec.state_dict().items()\n",
    "        }\n",
    "        state_dict.update(encodec_parameters)\n",
    "    vocoder.load_state_dict(state_dict)\n",
    "    vocoder = vocoder.eval().to(device)\n",
    "\n",
    "    return vocoder\n",
    "\n",
    "vocoder = load_vocoder(True, local_path='/workspace/matchatrain/vocoder')\n",
    "# denoiser = Denoiser(vocoder, mode='zeros')\n",
    "\n",
    "def load_vocoder(checkpoint_path):\n",
    "    h = AttrDict(v1)\n",
    "    hifigan = HiFiGAN(h).to(device)\n",
    "    hifigan.load_state_dict(torch.load(checkpoint_path, map_location=device)['generator'])\n",
    "    _ = hifigan.eval()\n",
    "    hifigan.remove_weight_norm()\n",
    "    return hifigan\n",
    "\n",
    "# vocoder = load_vocoder('/workspace/generator_v1')\n",
    "# denoiser = Denoiser(vocoder, mode='zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc2ba0-09ff-40e2-9e60-6b77b534f9fb",
   "metadata": {},
   "source": [
    "### Helper functions to synthesise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a1879-24fd-4757-849c-850339120796",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def process_text(text: str):\n",
    "    x = torch.tensor(intersperse(text_to_sequence(text, ['english_cleaners2'])[0], 0),dtype=torch.long, device=device)[None]\n",
    "    x_lengths = torch.tensor([x.shape[-1]],dtype=torch.long, device=device)\n",
    "    x_phones = sequence_to_text(x.squeeze(0).tolist())\n",
    "    return {\n",
    "        'x_orig': text, # Hi how are you today?\n",
    "        'x': x,         # ids of phoneme embedding\n",
    "        'x_lengths': x_lengths,\n",
    "        'x_phones': x_phones # _h_ˈ_a_ɪ_ _h_ˌ_a_ʊ_ _ɑ_ː_ɹ_ _j_u_ː_ _t_ə_d_ˈ_e_ɪ_?_\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def synthesise(text, spks=None):\n",
    "    text_processed = process_text(text)\n",
    "    \n",
    "    start_t = dt.datetime.now()\n",
    "    output = model.synthesise(\n",
    "        text_processed['x'], \n",
    "        text_processed['x_lengths'],\n",
    "        n_timesteps=n_timesteps,\n",
    "        temperature=temperature,\n",
    "        spks=spks,\n",
    "        length_scale=length_scale\n",
    "    )\n",
    "    # merge everything to one dict    \n",
    "    output.update({'start_t': start_t, **text_processed})\n",
    "    return output\n",
    "\n",
    "@torch.inference_mode()\n",
    "def to_waveform(mel, vocoder):\n",
    "    audio = vocoder.decode(mel).clamp(-1, 1)\n",
    "    audio = denoiser(audio.squeeze(0), strength=0.00025).cpu().squeeze()\n",
    "    return audio.cpu().squeeze()\n",
    "    \n",
    "def save_to_folder(filename: str, output: dict, folder: str):\n",
    "    folder = Path(folder)\n",
    "    folder.mkdir(exist_ok=True, parents=True)\n",
    "    np.save(folder / f'{filename}', output['mel'].cpu().numpy())\n",
    "    sf.write(folder / f'{filename}.wav', output['waveform'], 22050, 'PCM_24')\n",
    "\n",
    "print(process_text(\"Hi how are you today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f857e3-2ef7-4c86-b776-596c4d3cf875",
   "metadata": {},
   "source": [
    "## Setup text to synthesise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a9acd-0845-4192-ba09-b9683e28a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The Secret Service believed that it was very doubtful that any President would ride regularly in a vehicle with a fixed top, even though transparent.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da9e2d-99b9-4c6f-8a08-c828e2cba121",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d216e5-4895-4da8-9d24-9e61021d2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of ODE Solver steps\n",
    "n_timesteps = 10\n",
    "\n",
    "## Changes to the speaking rate\n",
    "length_scale=1.0\n",
    "\n",
    "## Sampling temperature\n",
    "temperature = 0.667"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93aac89-c7f8-4975-8510-4e763c9689f4",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a227963-aa12-43b9-a706-1168b6fc0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "outputs, rtfs = [], []\n",
    "rtfs_w = []\n",
    "for i, text in enumerate(tqdm(texts)):\n",
    "    st = time.time()\n",
    "    output = synthesise(text) #, torch.tensor([15], device=device, dtype=torch.long).unsqueeze(0))\n",
    "    mel_100 = F.interpolate(output['mel'].unsqueeze(dim=1), size=(100, output['mel'].size(-1)), mode=\"bilinear\", align_corners=False).squeeze(dim=1)\n",
    "    print(f\"Only model [{time.time() - st}]\")\n",
    "    output['waveform'] = to_waveform(mel_100, vocoder)\n",
    "    print(f\"With vocoder [{time.time() - st}]\")\n",
    "\n",
    "    # Compute Real Time Factor (RTF) with HiFi-GAN\n",
    "    t = (dt.datetime.now() - output['start_t']).total_seconds()\n",
    "    rtf_w = t * 22050 / (output['waveform'].shape[-1])\n",
    "\n",
    "    ## Pretty print\n",
    "    print(f\"{'*' * 53}\")\n",
    "    print(f\"Input text - {i}\")\n",
    "    print(f\"{'-' * 53}\")\n",
    "    print(output['x_orig'])\n",
    "    print(f\"{'*' * 53}\")\n",
    "    print(f\"Phonetised text - {i}\")\n",
    "    print(f\"{'-' * 53}\")\n",
    "    print(output['x_phones'])\n",
    "    print(f\"{'*' * 53}\")\n",
    "    print(f\"RTF:\\t\\t{output['rtf']:.6f}\")\n",
    "    print(f\"RTF Waveform:\\t{rtf_w:.6f}\")\n",
    "    rtfs.append(output['rtf'])\n",
    "    rtfs_w.append(rtf_w)\n",
    "\n",
    "    ## Display the synthesised waveform\n",
    "    ipd.display(ipd.Audio(output['waveform'], rate=22050))\n",
    "\n",
    "    ## Save the generated waveform\n",
    "    save_to_folder(i, output, OUTPUT_FOLDER)\n",
    "\n",
    "print(f\"Number of ODE steps: {n_timesteps}\")\n",
    "print(f\"Mean RTF:\\t\\t\\t\\t{np.mean(rtfs):.6f} ± {np.std(rtfs):.6f}\")\n",
    "print(f\"Mean RTF Waveform (incl. vocoder):\\t{np.mean(rtfs_w):.6f} ± {np.std(rtfs_w):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e85c3f-1623-4647-b40c-fa96907656fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bcac6-6e32-4915-8531-446e2e80b5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5163d-4543-454d-bae3-bd6243ba4402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf29b86-39d3-425c-8946-0e7c9708387e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a2b82-0615-442e-9f0b-f63ea7092248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
