{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f4bc0-c28e-4670-9a5e-4c7928ab8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5876c0-b47e-4c80-9e9c-62550f81b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Hifigan imports\n",
    "from matcha.hifigan.config import v1\n",
    "from matcha.hifigan.denoiser import Denoiser\n",
    "from matcha.hifigan.env import AttrDict\n",
    "from matcha.hifigan.models import Generator as HiFiGAN\n",
    "# Matcha imports\n",
    "from matcha.models.matcha_tts import MatchaTTS\n",
    "from matcha.text import sequence_to_text, text_to_sequence\n",
    "from matcha.utils.model import denormalize\n",
    "from matcha.utils.utils import get_user_data_dir, intersperse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a30306-588c-4f22-8d9b-e2676880b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# This allows for real time code changes being reflected in the notebook, no need to restart the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3b3c3-d014-443b-84eb-e143cdec3e21",
   "metadata": {},
   "source": [
    "## Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c8648-b072-4714-b56f-61f4e97bdd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "\n",
    "def load_hparams_from_ckpt(ckpt_path: str):\n",
    "    # 1) 안전 목록에 OmegaConf 타입 허용\n",
    "    torch.serialization.add_safe_globals([DictConfig, ListConfig])\n",
    "    try:\n",
    "        # 2) 우선 안전하게 weights_only=True로 시도\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "    except Exception as e1:\n",
    "        # 3) 신뢰 가능한 파일일 때만 전체 언피클 허용\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "    # 4) 다양한 키 호환\n",
    "    hparams = (\n",
    "        ckpt.get(\"hyper_parameters\")\n",
    "        or ckpt.get(\"hparams\")\n",
    "        or ckpt.get(\"hparams_initial\")\n",
    "        or {}\n",
    "    )\n",
    "\n",
    "    # 5) DictConfig -> dict\n",
    "    if isinstance(hparams, (DictConfig, ListConfig)):\n",
    "        hparams = OmegaConf.to_container(hparams, resolve=True)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MATCHA_CHECKPOINT = \"/workspace/last.ckpt\"\n",
    "OUTPUT_FOLDER = \"synth_output\"\n",
    "\n",
    "# === 사용 예시 ===\n",
    "hparams = load_hparams_from_ckpt(MATCHA_CHECKPOINT)\n",
    "\n",
    "# 보통 이렇게 바로 인자로 들어갑니다.\n",
    "# model_empty = MatchaTTS(**hparams)   # 가중치 로드 안 함\n",
    "# model_empty.eval()\n",
    "# print(f\"Empty model! Parameter count: {count_params(model_empty)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e374bfd2-6c94-48e0-ba44-7875c99a034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    model = MatchaTTS.load_from_checkpoint(checkpoint_path, map_location=device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "count_params = lambda x: f\"{sum(p.numel() for p in x.parameters()):,}\"\n",
    "\n",
    "model = load_model(MATCHA_CHECKPOINT)\n",
    "print(f\"Model loaded! Parameter count: {count_params(model)}\") # 18.2M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077b84b-e3b6-42e1-a84b-2f7084b13f92",
   "metadata": {},
   "source": [
    "## Load HiFi-GAN (Vocoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b68184-968d-4868-9029-f0c40e9e68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocos import Vocos\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "import torch\n",
    "\n",
    "def load_vocoder(checkpoint_path):\n",
    "    h = AttrDict(v1)\n",
    "    hifigan = HiFiGAN(h).to(device)\n",
    "    hifigan.load_state_dict(torch.load(checkpoint_path, map_location=device)['generator'])\n",
    "    _ = hifigan.eval()\n",
    "    hifigan.remove_weight_norm()\n",
    "    return hifigan\n",
    "\n",
    "vocoder = load_vocoder('/workspace/generator_v1')\n",
    "denoiser = Denoiser(vocoder, mode='zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a1879-24fd-4757-849c-850339120796",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def process_text(text: str):\n",
    "    x = torch.tensor(intersperse(text_to_sequence(text, ['english_cleaners2'])[0], 0),dtype=torch.long, device=device)[None]\n",
    "    x_lengths = torch.tensor([x.shape[-1]],dtype=torch.long, device=device)\n",
    "    x_phones = sequence_to_text(x.squeeze(0).tolist())\n",
    "    return {\n",
    "        'x_orig': text, # Hi how are you today?\n",
    "        'x': x,         # ids of phoneme embedding\n",
    "        'x_lengths': x_lengths,\n",
    "        'x_phones': x_phones # _h_ˈ_a_ɪ_ _h_ˌ_a_ʊ_ _ɑ_ː_ɹ_ _j_u_ː_ _t_ə_d_ˈ_e_ɪ_?_\n",
    "    }\n",
    "\n",
    "@torch.inference_mode()\n",
    "def synthesise(text, spks=None):\n",
    "    text_processed = process_text(text)\n",
    "    \n",
    "    start_t = dt.datetime.now()\n",
    "    output = model.synthesise(\n",
    "        text_processed['x'], \n",
    "        text_processed['x_lengths'],\n",
    "        n_timesteps=n_timesteps,\n",
    "        temperature=temperature,\n",
    "        spks=spks,\n",
    "        length_scale=length_scale\n",
    "    )\n",
    "    # merge everything to one dict    \n",
    "    output.update({'start_t': start_t, **text_processed})\n",
    "    return output\n",
    "\n",
    "@torch.inference_mode()\n",
    "def to_waveform(mel, vocoder):\n",
    "    audio = vocoder(mel).clamp(-1, 1)\n",
    "    audio = denoiser(audio.squeeze(0), strength=0.00025).cpu().squeeze()\n",
    "    return audio.cpu().squeeze()\n",
    "    \n",
    "def save_to_folder(filename: str, output: dict, folder: str):\n",
    "    folder = Path(folder)\n",
    "    folder.mkdir(exist_ok=True, parents=True)\n",
    "    np.save(folder / f'{filename}', output['mel'].cpu().numpy())\n",
    "    sf.write(folder / f'{filename}.wav', output['waveform'], 22050, 'PCM_24')\n",
    "\n",
    "print(process_text(\"Hi how are you today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f857e3-2ef7-4c86-b776-596c4d3cf875",
   "metadata": {},
   "source": [
    "## Setup text to synthesise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a9acd-0845-4192-ba09-b9683e28a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv(\"/workspace/matchatrain/LJSpeech-1.1/test.csv\", sep=\"|\", header=None)\n",
    "print(len(eval_df))\n",
    "print(eval_df.iloc[0])\n",
    "\n",
    "texts =  [eval_df.iloc[i][1] for i in range(len(eval_df))]\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d216e5-4895-4da8-9d24-9e61021d2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "wmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "wmodel.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=wmodel,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "## Number of ODE Solver steps\n",
    "n_timesteps = 10\n",
    "\n",
    "## Changes to the speaking rate\n",
    "length_scale=1.0\n",
    "\n",
    "## Sampling temperature\n",
    "temperature = 0.667"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93aac89-c7f8-4975-8510-4e763c9689f4",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb66d8-0a19-451b-bc2c-c7680333727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "# 영어용 간단 정규화 (소문자, 기호 제거, 공백 정리)\n",
    "_punct = re.compile(r\"[^\\w\\s]\")\n",
    "def _normalize(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = _punct.sub(\" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _lev(a, b):  # Levenshtein distance\n",
    "    n, m = len(a), len(b)\n",
    "    dp = list(range(m+1))\n",
    "    for i in range(1, n+1):\n",
    "        prev, dp[0] = dp[0], i\n",
    "        for j in range(1, m+1):\n",
    "            cur = dp[j]\n",
    "            cost = 0 if a[i-1] == b[j-1] else 1\n",
    "            dp[j] = min(dp[j] + 1, dp[j-1] + 1, prev + cost)\n",
    "            prev = cur\n",
    "    return dp[m]\n",
    "\n",
    "def wer_en(ref: str, hyp: str) -> float:\n",
    "    r = _normalize(ref).split()\n",
    "    h = _normalize(hyp).split()\n",
    "    if not r:\n",
    "        return 0.0 if not h else 1.0\n",
    "    return _lev(r, h) / len(r)\n",
    "\n",
    "def cer_en(ref: str, hyp: str) -> float:\n",
    "    r = list(_normalize(ref).replace(\" \", \"\"))\n",
    "    h = list(_normalize(hyp).replace(\" \", \"\"))\n",
    "    if not r:\n",
    "        return 0.0 if not h else 1.0\n",
    "    return _lev(r, h) / len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a227963-aa12-43b9-a706-1168b6fc0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=16000, new_freq=22050)\n",
    "\n",
    "wer_list, cer_list = [], []\n",
    "outputs, rtfs = [], []\n",
    "rtfs_w = []\n",
    "for i, text in enumerate(tqdm(texts)):\n",
    "    st = time.time()\n",
    "    output = synthesise(text) #, torch.tensor([15], device=device, dtype=torch.long).unsqueeze(0))\n",
    "    waveform = to_waveform(output['mel'], vocoder)\n",
    "    mono_16k = resampler(waveform)\n",
    "    \n",
    "    script = pipe(mono_16k)\n",
    "    # print(text, '\\n')\n",
    "    # print(script['text'], '\\n\\n')\n",
    "\n",
    "    _w = wer_en(text, script['text'])\n",
    "    _c = cer_en(text, script['text'])\n",
    "    wer_list.append(_w)\n",
    "    cer_list.append(_c)\n",
    "\n",
    "    # display(Audio(waveform.cpu().detach().numpy(), rate=22050))\n",
    "\n",
    "    if i%50==49:\n",
    "        print(f\"\\nMean WER: {sum(wer_list)/len(wer_list):.4f}\")\n",
    "        print(f\"Mean CER: {sum(cer_list)/len(cer_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bcac6-6e32-4915-8531-446e2e80b5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5163d-4543-454d-bae3-bd6243ba4402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf29b86-39d3-425c-8946-0e7c9708387e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a2b82-0615-442e-9f0b-f63ea7092248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
