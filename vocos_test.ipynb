{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4dab76-d1cb-4fcb-9288-5d7bd7e53145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocos import Vocos\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "import torch\n",
    "\n",
    "# load vocoder\n",
    "def load_vocoder(is_local=False, local_path=\"\", device=\"cuda\", hf_cache_dir=None):\n",
    "    # vocoder = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(device)\n",
    "    if is_local:\n",
    "        print(f\"Load vocos from local path {local_path}\")\n",
    "        config_path = f\"{local_path}/config.yaml\"\n",
    "        model_path = f\"{local_path}/pytorch_model.bin\"\n",
    "    else:\n",
    "        print(\"Download Vocos from huggingface charactr/vocos-mel-24khz\")\n",
    "        repo_id = \"charactr/vocos-mel-24khz\"\n",
    "        config_path = hf_hub_download(repo_id=repo_id, cache_dir=hf_cache_dir, filename=\"config.yaml\")\n",
    "        model_path = hf_hub_download(repo_id=repo_id, cache_dir=hf_cache_dir, filename=\"pytorch_model.bin\")\n",
    "    vocoder = Vocos.from_hparams(config_path)\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
    "    from vocos.feature_extractors import EncodecFeatures\n",
    "\n",
    "    if isinstance(vocoder.feature_extractor, EncodecFeatures):\n",
    "        encodec_parameters = {\n",
    "            \"feature_extractor.encodec.\" + key: value\n",
    "            for key, value in vocoder.feature_extractor.encodec.state_dict().items()\n",
    "        }\n",
    "        state_dict.update(encodec_parameters)\n",
    "    vocoder.load_state_dict(state_dict)\n",
    "    vocoder = vocoder.eval().to(device)\n",
    "\n",
    "    return vocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37949f-92fe-43ff-a723-95bfebc9359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voco = load_vocoder(True, local_path='/workspace/matchatrain/vocoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75dad91-c54a-4728-bb1c-b83c0fd72929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c339cbf-a911-48d5-8b3a-db9a4e7e1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio, soundfile as sf\n",
    "from vocos import Vocos\n",
    "import matplotlib.pyplot as plt\n",
    "from audiotools import AudioSignal\n",
    "import time\n",
    "\n",
    "wav_path = \"/workspace/matchatrain/gpt-4o-mini.mp3\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "y, sr = torchaudio.load(wav_path)\n",
    "if y.size(0) > 1:\n",
    "    y = y.mean(dim=0, keepdim=True)\n",
    "if sr != 24000:\n",
    "    y = torchaudio.functional.resample(y, sr, 24000)\n",
    "sr = 24000\n",
    "\n",
    "# 2) 멜 스펙트로그램 (24kHz, n_mels=100, hop=256)\n",
    "mel_fn = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sr, n_fft=1024, hop_length=256, win_length=1024,\n",
    "    f_min=0.0, f_max=sr/2, n_mels=100, power=1.0, center=True, norm=None\n",
    ")\n",
    "mel = mel_fn(y)                                 # [1, 100, T]\n",
    "mel = torch.log(mel.clamp(min=1e-5))            # log-mel\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(mel[0].cpu().numpy(), aspect='auto', origin='lower',\n",
    "           interpolation='none')\n",
    "plt.colorbar(label=\"Log-Mel energy\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Mel bins (100)\")\n",
    "plt.title(\"Log-Mel Spectrogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# vocos = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(device).eval()\n",
    "st = time.time()\n",
    "with torch.inference_mode():\n",
    "    wav_hat = voco.decode(mel.to(device))      # [1, samples]\n",
    "print(time.time() - st)\n",
    "\n",
    "sf.write(\"reconstructed.wav\", wav_hat[0].cpu().numpy(), sr)\n",
    "AudioSignal(wav_hat[0].cpu().numpy(), sample_rate=sr).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd8486-b375-4f9f-9304-806e0d15f1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
